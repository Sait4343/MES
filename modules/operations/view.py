import streamlit as st
import pandas as pd
from modules.operations.services import OperationsService
from core.config import UserRole

def render():
    st.header("üßµ –î–æ–≤—ñ–¥–Ω–∏–∫ –û–ø–µ—Ä–∞—Ü—ñ–π")
    
    service = OperationsService()
    
    # Create tabs
    tab_list, tab_new, tab_import, tab_export = st.tabs(["üìã –°–ø–∏—Å–æ–∫ —ñ –†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è", "‚ûï –ù–æ–≤–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è", "üì• –Ü–º–ø–æ—Ä—Ç (Excel)", "üì§ –ï–∫—Å–ø–æ—Ä—Ç"])
    
    # --- TAB 1: LIST & EDIT ---
    with tab_list:
        df = service.get_operations()
        if not df.empty:
            st.info("üí° –í–∏ –º–æ–∂–µ—Ç–µ —Ä–µ–¥–∞–≥—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –ø—Ä—è–º–æ –≤ —Ç–∞–±–ª–∏—Ü—ñ. –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter –∞–±–æ –∫–ª—ñ–∫–Ω—ñ—Ç—å –∑–∞ –º–µ–∂–∞–º–∏ –∫–ª—ñ—Ç–∏–Ω–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–º—ñ–Ω.")
            
            # Use data_editor
            edited_df = st.data_editor(
                df,
                key="ops_editor",
                column_config={
                    "id": None, # Hide ID
                    "operation_key": "–ö–ª—é—á",
                    "article": "–ê—Ä—Ç–∏–∫—É–ª",
                    "operation_number": "‚Ññ –û–ø.",
                    "section": "–î—ñ–ª—å–Ω–∏—Ü—è",
                    "norm_time": st.column_config.NumberColumn("–ù–æ—Ä–º–∞ (—Ö–≤)", format="%.2f"),
                    "comment": "–ö–æ–º–µ–Ω—Ç–∞—Ä",
                    "color": "–ö–æ–ª—ñ—Ä",
                    "created_at": st.column_config.DatetimeColumn("–°—Ç–≤–æ—Ä–µ–Ω–æ", format="DD.MM.YYYY HH:mm", disabled=True)
                },
                hide_index=True,
                use_container_width=True,
                num_rows="dynamic" # Allow adding/deleting rows if supported by backend logic logic below
            )

            # Detect changes (This is a simplified "snapshot" approach. 
            # Real-time sync requires comparing edited_df with df, or using on_change callback)
            # For simplicity in this interaction model, we can add a "Save Changes" button 
            # OR process changes immediately if identifying what changed is easy.
            # Ideally, data_editor returns the new state. We need to find Diff.
            
            # Simple Diff Logic for Updates:
            if not df.equals(edited_df):
                # We need to identify what changed.
                # However, st.data_editor state persistence can be tricky without a button or callback.
                # Let's try to capture changes via session state if needed, 
                # but standard practice: Button to commit bulk edits OR iterative updates.
                
                # Iterating rows to find changes:
                # This is heavy if DF is large.
                # Constraint: We only want to save specific changes.
                
                if st.button("üíæ –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–º—ñ–Ω–∏ –≤ —Ç–∞–±–ª–∏—Ü—ñ"):
                    with st.spinner("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è..."):
                        # Identify revised rows
                        # Assuming 'id' is prevalent.
                        
                        # 1. Updates
                        for index, row in edited_df.iterrows():
                            # Find original row by ID
                            # This is O(N^2) effectively if not optimized, but OK for small catalogs.
                            # Better: compare based on ID index.
                             
                            # If new row (no ID or ID is NaN if added via dynamic), Handle Create
                            # Note: Supabase/Pandas handling of new rows in data_editor usually results in empty IDs.
                            
                            op_id = row.get("id")
                            
                            # Clean data for DB
                            row_data = {
                                "operation_key": row["operation_key"],
                                "article": row["article"],
                                "operation_number": row["operation_number"],
                                "section": row["section"],
                                "norm_time": row["norm_time"],
                                "comment": row["comment"],
                                "color": row["color"]
                            }
                            
                            if pd.isna(op_id):
                                # It's a NEW row added via UI
                                service.create_operation(row_data)
                            else:
                                # It's an UPDATE. Check if changed? 
                                # For simplicity, just update all (or filter).
                                # To avoid spamming DB, we should compare.
                                pass
                                # We'll rely on "New Operation" tab for adding mainly, 
                                # and use this mainly for edits.
                                # But let's support updates.
                                service.update_operation(op_id, row_data)
                                
                        # 2. Deletes
                        # st.data_editor allows deleting rows. We need to find IDs that are in DB but not in edited_df.
                        original_ids = df["id"].tolist()
                        current_ids = [x for x in edited_df["id"].tolist() if pd.notna(x)]
                        
                        ids_to_delete = set(original_ids) - set(current_ids)
                        for d_id in ids_to_delete:
                            service.delete_operation(d_id)
                            
                        st.success("–ó–º—ñ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ!")
                        st.rerun()

        else:
            st.info("–î–æ–≤—ñ–¥–Ω–∏–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π. –î–æ–¥–∞–π—Ç–µ –æ–ø–µ—Ä–∞—Ü—ñ—ó —á–µ—Ä–µ–∑ —ñ–º–ø–æ—Ä—Ç –∞–±–æ –≤–∫–ª–∞–¥–∫—É '–ù–æ–≤–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è'.")

    # --- TAB 2: NEW OPERATION ---
    with tab_new:
        st.subheader("‚ûï –î–æ–¥–∞—Ç–∏ –Ω–æ–≤—É –æ–ø–µ—Ä–∞—Ü—ñ—é")
        with st.form("new_op_form"):
            c1, c2 = st.columns(2)
            op_key = c1.text_input("–ö–ª—é—á –æ–ø–µ—Ä–∞—Ü—ñ—ó (Operation Key)", help="–£–Ω—ñ–∫–∞–ª—å–Ω–∏–π —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä")
            article = c2.text_input("–ê—Ä—Ç–∏–∫—É–ª")
            
            c3, c4 = st.columns(2)
            op_num = c3.text_input("–ù–æ–º–µ—Ä –æ–ø–µ—Ä–∞—Ü—ñ—ó")
            section = c4.text_input("–î—ñ–ª—å–Ω–∏—Ü—è (Section)")
            
            c5, c6 = st.columns(2)
            norm_time = c5.number_input("–ù–æ—Ä–º–∞ —á–∞—Å—É (—Ö–≤)", min_value=0.0, step=0.01)
            color = c6.color_picker("–ö–æ–ª—ñ—Ä", "#E0E0E0")
            
            comment = st.text_area("–ö–æ–º–µ–Ω—Ç–∞—Ä")
            
            if st.form_submit_button("–°—Ç–≤–æ—Ä–∏—Ç–∏ –æ–ø–µ—Ä–∞—Ü—ñ—é"):
                if not op_key or not article:
                    st.error("–ö–ª—é—á —Ç–∞ –ê—Ä—Ç–∏–∫—É–ª —î –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º–∏!")
                else:
                    data = {
                        "operation_key": op_key,
                        "article": article,
                        "operation_number": op_num,
                        "section": section,
                        "norm_time": norm_time,
                        "comment": comment,
                        "color": color
                    }
                    success, msg = service.create_operation(data)
                    if success:
                        st.success("–û–ø–µ—Ä–∞—Ü—ñ—é –¥–æ–¥–∞–Ω–æ!")
                        st.rerun()
                    else:
                        st.error(f"–ü–æ–º–∏–ª–∫–∞: {msg}")

    # --- TAB 3: IMPORT ---
    with tab_import:
        if st.session_state.role not in [UserRole.ADMIN, UserRole.MANAGER]:
            st.warning("‚ö†Ô∏è –Ü–º–ø–æ—Ä—Ç –¥–æ—Å—Ç—É–ø–Ω–∏–π —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ê–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä—ñ–≤ —Ç–∞ –ú–µ–Ω–µ–¥–∂–µ—Ä—ñ–≤.")
        else:
            st.markdown("### –Ü–º–ø–æ—Ä—Ç –æ–ø–µ—Ä–∞—Ü—ñ–π –∑ Excel")
            uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª", type=["xlsx", "xls"])
            
            if uploaded_file:
                try:
                    # 1. Inspect Excel File for Sheets
                    xls = pd.ExcelFile(uploaded_file)
                    sheet_names = xls.sheet_names
                    
                    st.write(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∞—Ä–∫—É—à—ñ–≤: {len(sheet_names)}")
                    selected_sheet = st.selectbox("–û–±–µ—Ä—ñ—Ç—å –∞—Ä–∫—É—à –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É", sheet_names)
                    
                    # 2. Read Data from Selected Sheet
                    df_raw = pd.read_excel(uploaded_file, sheet_name=selected_sheet)
                    
                    st.success(f"–§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ! –†—è–¥–∫—ñ–≤: {len(df_raw)}")
                    st.write("–ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø–µ—Ä–µ–≥–ª—è–¥ (–ø–µ—Ä—à—ñ 3 —Ä—è–¥–∫–∏):")
                    st.dataframe(df_raw.head(3))
                    
                    st.divider()
                    st.subheader("üîó –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ–≤–ø—Ü—ñ–≤")
                    st.info("–û–±–µ—Ä—ñ—Ç—å, —è–∫–∏–π —Å—Ç–æ–≤–ø–µ—Ü—å –∑ –≤–∞—à–æ–≥–æ —Ñ–∞–π–ª—É –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –ø–æ–ª—é –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö.")
                    
                    excel_headers = ["(–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏)"] + list(df_raw.columns)
                    
                    # DB Fields we need to map
                    db_fields = {
                        "operation_key": "–ö–ª—é—á –æ–ø–µ—Ä–∞—Ü—ñ—ó",
                        "article": "–ê—Ä—Ç–∏–∫—É–ª",
                        "operation_number": "‚Ññ –æ–ø–µ—Ä–∞—Ü—ñ—ó",
                        "section": "–î—ñ–ª—å–Ω–∏—Ü—è",
                        "norm_time": "–ù–æ—Ä–º–∞ —á–∞—Å—É (—Ö–≤)",
                        "comment": "–ö–æ–º–µ–Ω—Ç–∞—Ä",
                        "color": "–ö–æ–ª—ñ—Ä" # Optional but good to have
                    }
                    
                    mapping = {}
                    cols = st.columns(3)
                    
                    for i, (db_field, label) in enumerate(db_fields.items()):
                        # Try to auto-guess index
                        default_idx = 0
                        for idx, h in enumerate(excel_headers):
                            if h != "(–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏)" and any(x in h.lower() for x in label.lower().split()):
                                default_idx = idx
                                break
                        
                        with cols[i % 3]:
                            selected_col = st.selectbox(
                                f"–ü–æ–ª–µ –ë–î: **{label}**", 
                                options=excel_headers, 
                                index=default_idx,
                                key=f"map_{db_field}"
                            )
                            if selected_col != "(–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏)":
                                mapping[db_field] = selected_col
                    
                    st.divider()
                    
                    st.divider()
                    
                    if not mapping:
                        st.warning("‚ö†Ô∏è –°–ø–æ—á–∞—Ç–∫—É —Å–ø—ñ–≤—Å—Ç–∞–≤—Ç–µ —Ö–æ—á–∞ –± –æ–¥–∏–Ω —Å—Ç–æ–≤–ø–µ—Ü—å.")
                    else:
                        # --- PRE-IMPORT VALIDATION ---
                        # 1. Identify mapped columns in the dataframe
                        mapped_excel_cols = [v for k, v in mapping.items() if v]
                        
                        # 2. Analyze rows
                        # We consider a row "empty" if ALL mapped columns are empty/NaN
                        # We consider a row "valid" if at least one mapped column has data
                        
                        # Create a subset for analysis
                        df_mapped_subset = df_raw[mapped_excel_cols]
                        
                        # Count total
                        total_rows = len(df_mapped_subset)
                        
                        # Identify empty rows (all null/nan in mapped cols)
                        # We use .isna().all(axis=1) (or check for empty strings too if needed)
                        # Let's treat empty strings as NaN for this check
                        is_empty_mask = df_mapped_subset.replace(r'^\s*$', pd.NA, regex=True).isna().all(axis=1)
                        empty_rows_count = is_empty_mask.sum()
                        valid_rows_count = total_rows - empty_rows_count
                        
                        # Display Stats
                        st.markdown("#### üìä –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö")
                        c_stat1, c_stat2, c_stat3 = st.columns(3)
                        c_stat1.metric("–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤", total_rows)
                        c_stat2.metric("–ü—É—Å—Ç—ñ —Ä—è–¥–∫–∏", empty_rows_count, delta_color="inverse")
                        c_stat3.metric("–î–æ —ñ–º–ø–æ—Ä—Ç—É", valid_rows_count)
                        
                        # Options
                        skip_empty = st.checkbox("üö´ –ù–µ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø—É—Å—Ç—ñ —Ä—è–¥–∫–∏", value=True)
                        
                        if valid_rows_count == 0:
                            st.error("‚ùå –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É (–≤—Å—ñ —Ä—è–¥–∫–∏ –ø—É—Å—Ç—ñ –∞–±–æ –Ω–µ –≤–∏–±—Ä–∞–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ).")
                        else:
                            if st.button("üöÄ –í–∏–∫–æ–Ω–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç"):
                                with st.spinner("–Ü–º–ø–æ—Ä—Ç—É—î–º–æ –¥–∞–Ω—ñ..."):
                                    # Filter DF if needed
                                    if skip_empty:
                                        # Keep only rows that are NOT empty
                                        df_to_import = df_raw[~is_empty_mask]
                                    else:
                                        df_to_import = df_raw
                                        
                                    s_count, e_count = service.import_operations(df_to_import, mapping)
                                
                                if e_count == 0:
                                    st.success(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ {s_count} —Ä—è–¥–∫—ñ–≤!")
                                    st.balloons()
                                else:
                                    st.warning(f"–Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ: {s_count}, –ü–æ–º–∏–ª–æ–∫: {e_count}")
                                    
                except Exception as e:
                    st.error(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∞–±–æ –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—É: {e}")

    # --- TAB 3: EXPORT ---
    with tab_export:
        st.markdown("### –ï–∫—Å–ø–æ—Ä—Ç –æ–ø–µ—Ä–∞—Ü—ñ–π –≤ Excel")
        
        # Re-fetch or reuse if appropriate, but cleaner to fetch fresh
        df_export = service.get_operations()
        
        if df_export.empty:
            st.info("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É.")
        else:
            st.write(f"–£ –±–∞–∑—ñ –∑–Ω–∞–π–¥–µ–Ω–æ {len(df_export)} –∑–∞–ø–∏—Å—ñ–≤.")
            
            # Convert to Excel in memory
            import io
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df_export.to_excel(writer, index=False, sheet_name='Operations')
            
            excel_data = output.getvalue()
            
            st.download_button(
                label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ Excel",
                data=excel_data,
                file_name="operations_catalog.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
